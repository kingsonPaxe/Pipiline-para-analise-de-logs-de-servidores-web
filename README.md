# An√°lise de Logs de Servidor Web

Um projeto completo de an√°lise e visualiza√ß√£o de logs de servidor web, utilizando t√©cnicas de ETL (Extract, Transform, Load) para processar, enriquecer e analisar dados de acesso a servidores.

## Descri√ß√£o do Projeto

Este projeto implementa um pipeline de dados que coleta logs de acesso HTTP bruto, realiza limpeza e enriquecimento dos dados utilizando geolocaliza√ß√£o e an√°lise de user-agents, e disponibiliza as informa√ß√µes atrav√©s de um dashboard interativo com Streamlit.

**Principais funcionalidades:**
- Extra√ß√£o e parsing de logs de servidor (HTTP)
- Limpeza e valida√ß√£o de dados
- Enriquecimento com geolocaliza√ß√£o e informa√ß√µes de navegador
- Armazenamento em m√∫ltiplos formatos (Parquet, SQLite, Pickle)
- Dashboard interativo com m√©tricas e visualiza√ß√µes em Plotly
- An√°lise explorat√≥ria com Jupyter Notebooks

---

## üìÅ Estrutura do Projeto

```
Analise de logs de servidor web/
‚îú‚îÄ‚îÄ accessLog.csv              # Logs brutos de acesso HTTP
‚îú‚îÄ‚îÄ Ips.csv                    # Dados enriquecidos de geolocaliza√ß√£o de IPs
‚îú‚îÄ‚îÄ access.log                 # Arquivo de log original do servidor
‚îú‚îÄ‚îÄ log_dw.pkl                 # Dataset processado (pickle format)
‚îú‚îÄ‚îÄ logServidores_web.db       # Banco de dados SQLite com dados processados
‚îú‚îÄ‚îÄ An√°lise.ipynb              # Notebook com an√°lise explorat√≥ria de dados
‚îú‚îÄ‚îÄ index.ipynb                # Notebook adicional para an√°lises
‚îú‚îÄ‚îÄ dashboard.py               # Dashboard interativo com Streamlit
‚îú‚îÄ‚îÄ workflow.excalidraw        # Diagrama visual do pipeline de dados
‚îú‚îÄ‚îÄ requirements.txt           # Depend√™ncias Python
‚îî‚îÄ‚îÄ README.md                  # Este arquivo
```

---

## üîÑ Workflow do Projeto

```mermaid
graph LR
    A["üìÅ Raw Data<br/>access.log"] --> B["üêç Python Script<br/>Coleta e Valida√ß√£o"]
    B --> C["üîß Extract<br/>Regex & Pandas<br/>Parsing"]
    C --> D["‚öôÔ∏è Transform<br/>Limpeza & Enriquecimento<br/>Geolocaliza√ß√£o"]
    D --> E["üíæ Load<br/>Armazenamento"]
    E --> F["üìä Dashboard<br/>Streamlit App<br/>Gr√°ficos & M√©tricas"]
    
    E --> G["Parquet"]
    E --> H["SQLite3"]
    E --> I["Pickle"]
```

### Etapas do Pipeline:

1. **Extract** 
   - Leitura de logs brutos em formato HTTP
   - Parsing com express√µes regulares
   - Valida√ß√£o de campos

2. **Transform** 
   - Limpeza e normaliza√ß√£o de dados
   - Identifica√ß√£o de user-agents (navegadores, bots)
   - Detec√ß√£o de dispositivos (mobile, tablet, PC)
   - Enriquecimento com geolocaliza√ß√£o de IPs
   - C√°lculo de status codes e taxa de erros

3. **Load** 
   - Armazenamento em Parquet (formato colunar)
   - Persist√™ncia em SQLite3 para consultas SQL
   - Exporta√ß√£o em Pickle para an√°lises r√°pidas

4. **Visualiza√ß√£o** 
   - Dashboard interativo com Streamlit
   - Gr√°ficos interativos com Plotly
   - Filtros por per√≠odo, IP, URL e status

---

## üìä Dados Dispon√≠veis

### Arquivos de Dados

| Arquivo | Descri√ß√£o | Registros |
|---------|-----------|-----------|
| `accessLog.csv` | Logs completos com an√°lise de user-agents | ~100k+ registros |
| `Ips.csv` | Geolocaliza√ß√£o e informa√ß√µes de rede de IPs | Dados enriquecidos |
| `log_dw.pkl` | Dataset processado em formato Pickle | Dados limpos |

### Dados Extra√≠dos

**Do arquivo de log cada requisi√ß√£o cont√©m:**
- IP do cliente
- Data e hora
- M√©todo HTTP (GET, POST, etc.)
- URL acessada
- Protocolo (HTTP/1.1, HTTP/2, etc.)
- Status HTTP
- Device type (mobile, tablet, PC)
- Bot detection
- Navegador e SO
- Geolocaliza√ß√£o (pa√≠s, cidade, coordenadas)
- ISP e provedor de rede

### KPIs Principais

- **Total de requisi√ß√µes:** Contagem geral de acessos
- **IPs √∫nicos:** N√∫mero de clientes distintos
- **Taxa de erros (4xx/5xx):** Percentual de requisi√ß√µes com problemas
- **URL mais visitada:** Endpoint mais acessado
- **Distribui√ß√£o de status codes:** An√°lise de sucesso vs erros
- **Requisi√ß√µes por hor√°rio:** Padr√£o de acesso ao longo do dia
- **Requisi√ß√µes por pa√≠s:** Distribui√ß√£o geogr√°fica
- **Propor√ß√£o de bots:** Percentual de acesso automatizado

---

## üõ†Ô∏è Tecnologias e Depend√™ncias

### Stack Tecnol√≥gico

| Tecnologia | Vers√£o | Prop√≥sito |
|-----------|--------|----------|
| **Python** | 3.x | Linguagem principal |
| **Pandas** | 3.0.1 | Manipula√ß√£o de dados |
| **NumPy** | 2.4.2 | Computa√ß√µes num√©ricas |
| **Plotly** | 6.5.2 | Visualiza√ß√µes interativas |
| **Streamlit** | - | Framework para dashboard |
| **SQLAlchemy** | 2.0.46 | ORM para banco de dados |
| **Jupyter** | 9.10.0 | Notebooks interativos |
| **user-agents** | 2.2.0 | Parsing de user-agents |
| **ua-parser** | 1.0.1 | Parser de navegadores e SO |

**Todas as depend√™ncias podem ser instaladas com:**

```bash
pip install -r requirements.txt
```

---

## üöÄ Como Usar

### 1. Instala√ß√£o

```bash
# Clonar o reposit√≥rio
git clone <repositorio>
cd "Analise de logs de servidor web"

# Instalar depend√™ncias
pip install -r requirements.txt
```

### 2. An√°lise Explorat√≥ria (Jupyter)

Para analisar os dados interativamente:

```bash
jupyter notebook An√°lise.ipynb
```

O notebook `An√°lise.ipynb` cont√©m:
- Carregamento e explora√ß√£o dos dados
- KPIs principais
- Visualiza√ß√µes com Plotly
- An√°lise de tend√™ncias por hor√°rio
- Distribui√ß√£o geogr√°fica
- Segmenta√ß√£o de URLs
- Detec√ß√£o de padr√µes de acesso

### 3. Dashboard Interativo (Streamlit)

Para visualizar o dashboard em tempo real:

```bash
streamlit run dashboard.py
```

O dashboard oferece:
- Visualiza√ß√µes interativas
- Filtros din√¢micos
- Gr√°ficos de tend√™ncias
- Mapa geogr√°fico
- An√°lise de dispositivos
- Estat√≠sticas de bots

---

## üìà Caracter√≠sticas da An√°lise

### Visualiza√ß√µes Inclu√≠das

1. **Distribui√ß√£o de Status Codes**
   - Gr√°fico de pizza com sucesso vs erros
   - An√°lise detalhada de 4xx e 5xx

2. **Requisi√ß√µes por Hor√°rio**
   - Linha do tempo mostrando picos de acesso
   - Identifica√ß√£o de per√≠odos de maior atividade

3. **Top URLs Acessadas**
   - Ranking dos endpoints mais visitados
   - Volume de requisi√ß√µes por URL

4. **Distribui√ß√£o Geogr√°fica**
   - Mapa dos pa√≠ses de origem
   - Tabela com IPs √∫nicos por localiza√ß√£o

5. **An√°lise de Dispositivos**
   - Propor√ß√£o de mobile, tablet e desktop
   - Tend√™ncias de acesso por tipo de dispositivo

6. **Atividade de Bots**
   - Percentual de acesso de bots vs usu√°rios reais
   - Identifica√ß√£o de bots mais comuns

7. **An√°lise de Navegadores**
   - Navegadores mais utilizados
   - Sistemas operacionais

---

## üìù Estrutura dos Notebooks

### `An√°lise.ipynb`
Notebook principal com an√°lise completa dos dados:
- **Se√ß√£o 1:** Carregamento e informa√ß√µes dos dados
- **Se√ß√£o 2:** KPIs principais (total de requisi√ß√µes, IPs √∫nicos, taxa de erros)
- **Se√ß√£o 3:** An√°lise temporal (padr√µes por hora)
- **Se√ß√£o 4:** Geolocaliza√ß√£o e origem dos acessos
- **Se√ß√£o 5:** An√°lise de URLs e endpoints
- **Se√ß√£o 6:** Segmenta√ß√£o por dispositivo
- **Se√ß√£o 7:** Detec√ß√£o e an√°lise de bots

### `index.ipynb`
Notebook adicional com an√°lises complementares e experimenta√ß√µes.

---

## üí° Insights Poss√≠veis

Com esta an√°lise voc√™ pode:

- Identificar padr√µes de acesso ao servidor
- Descobrir origem geogr√°fica do tr√°fego
- Detectar atividade de bots e crawlers
- Entender quais dispositivos mais acessam o site
- Otimizar infraestrutura baseado em hor√°rios de pico
- Identificar tentativas de acesso indevido (4xx/5xx)
- Gerar relat√≥rios executivos de tr√°fego
- Comparar performance entre URLs

---

## üì¶ Formatos de Armazenamento

O projeto utiliza m√∫ltiplos formatos para diferentes necessidades:

### Pickle (`.pkl`)
- **Uso:** Carregamento r√°pido para an√°lises em Jupyter
- **Vantagem:** Preserva tipos de dados Python, carregamento instant√¢neo
- **Arquivo:** `log_dw.pkl`

### SQLite (`.db`)
- **Uso:** Consultas SQL, relat√≥rios din√¢micos
- **Vantagem:** Portabilidade, suporta queries complexas
- **Arquivo:** `logServidores_web.db`

### Parquet
- **Uso:** Armazenamento comprimido, analytics
- **Vantagem:** Compress√£o, leitura coluna-por-coluna eficiente
- **Ideal para:** Big data e an√°lises estat√≠sticas

---

## üîß Customiza√ß√µes Poss√≠veis

Voc√™ pode adaptar o projeto para:

- Adicionar novos filtros no dashboard
- Incluir an√°lise de performance (response time)
- Integrar alertas para anomalias
- Expandir an√°lise de seguran√ßa
- Gerar relat√≥rios autom√°ticos
- Adicionar machine learning para previs√µes
- Integrar com ferramentas de monitoramento

---

## üìã Requisitos do Sistema

- **Python:** 3.8+
- **Mem√≥ria RAM:** 2GB (m√≠nimo para an√°lise completa)
- **Espa√ßo em disco:** 500MB para dados + depend√™ncias
- **Navegador:** Qualquer navegador moderno para o dashboard

---

## üìß Informa√ß√µes Adicionais

Este projeto foi desenvolvido como parte de um est√°gio, com foco em:
- Engenharia de dados (ETL pipeline)
- An√°lise explorat√≥ria de dados (EDA)
- Visualiza√ß√£o de dados
- Desenvolvimento de dashboards

---

## üìÑ Archivos Especiais

- **`workflow.excalidraw`:** Diagrama visual do pipeline (abrir com Excalidraw)
- **`tabela operadores re.pdf`:** Refer√™ncia de express√µes regulares usadas na parsing

---

## Status do Projeto

- Pipeline ETL completo
- Dashboard funcional
- An√°lise explorat√≥ria
- Documenta√ß√£o
- Poss√≠veis melhorias e expans√µes

---

**Desenvolvido durante est√°gio**

*√öltima atualiza√ß√£o: Fevereiro de 2026*
